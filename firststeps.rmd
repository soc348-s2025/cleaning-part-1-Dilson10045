---
title: "Starting tidy text"
output: html_notebook
---

```{r}
library(gutenbergr)
library(tidytext)
library(janeaustenr)
library(tidyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(scales)
```

```{r eval = FALSE}
quakersaints <-  gutenberg_download(c(19605))
```

```{r}

tidy_quakersaints <- quakersaints |>
  unnest_tokens(word, text) 
```

View the tidied data. \## What does unnest_tokens() do?

```         
The unnest_tokens function splits a column into tokens, then flattering the table into a one token per row view.
```

One think that would be convenient is to be able to reconstruct where in the text the particular token came from. For example we could track the row number in the original data.

```{r}

tidy_quakersaints <- quakersaints |> 
  mutate(row = row_number()) |>
  unnest_tokens(word, text) 
```

## View the data. How does it differ?

```         
The data differs because now we can see where every single letter came from, from its respected row. For example in the `tidy_quakersaints` data set we can see that the word `note` came from row 2 of the `quakersaints` data set from the sentence "Transcriber's Note:" in row 2. One thing I did notice is that every single word is not capitalize at the beginning of the word.
```

Let's find the most common words

```{r}
tidy_quakersaints |>  count(word, sort = TRUE)
```

## What do you notice about the most common words? Are they interesting? Do they have anything in common?

```         
I noticed that the most common words come from run on sentences, for example the word `the` is almost always on run on sentences. Same can be said about and, to, and of.
```

In text data "stop words" are words that are common and not meaningful. They are words we don't want to include in our data.\
This is a judgement, but to keep it simple, let's use the stop word list from the tidy text package. This list comes from three different lexicons so we could pick one, but for our first try we'll use them all.

Use View() to look at the stop words.

```{r}

tidy_quakersaints <- quakersaints |>
   mutate(row = row_number()) |>
  unnest_tokens(word, text) |>
  anti_join(stop_words)
```

What is an anti-join?

```         
anti_join is a function that returns all rows from X without a match in Y. 
```

Notice that I use `|>` instead of `%>%` ... this is newer style and does not require loading dplyr or magrittr. It is part of base r.

```{r}
tidy_quakersaints |>  count(word, sort = TRUE)
```

How does this list of the most frequent words differ from the first one?

```         
This list differs from the first one because now we dont have all the common words from the 3 Lexicons which are Smart, Snowball, and onix. So if we go back to our tidy_quakersaints data set,  we wont see  common words such as a, able, around, be, you're , he's and so on on our token columns.
```
